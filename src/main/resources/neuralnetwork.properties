# ************************************************************************************************************ #
# ***********                                  COMMON PROPERTIES                                   *********** #
# ************************************************************************************************************ #
# the initializer function to initialize neural network matrices.
# available values: static|random|xavier|kaiming
initializer=random
# the learning rate must have a value between 0.0 and 1.0.
learning_rate=0.8
# the default rectifier as activation function for the neural network.
# available values: identity|relu|leaky_relu|sigmoid|sigmoid_accurate|silu|silu_accurate|tanh|elu|gelu|softplus|softmax.
rectifier=sigmoid
# the optimizer for the learning rate between iterations.
# available values: none|sgd
learning_rate_optimizer=sgd
# the learning rate decay as momentum.
# if learning_rate_optimizer is set to 'none', this value will have no effect.
# must have a value between 0.0 and 1.0.
learning_rate_momentum=0.01

# ************************************************************************************************************ #
# ***********                               SUPERVISED LEARNING ONLY                               *********** #
# ************************************************************************************************************ #
# the cost function is the metric for the error of one iteration. Its derivation is the loss.
# available values: mse_naive|mse|cross_entropy|exponential|hellinger_distance|kld|gkld|isd
cost_function=mse
# the regularizer will adapt the penalty by the loss function.
# available values: none|l1|l2|elastic
regularizer=none
# regularization functions may rely on a regularization parameter. this parameter, usually called lambda,
# will be controlled by the property regularizer_param.
regularizer_param=0
# dropout is an additional regularization technique. during training, it will set a certain percentage
# of output layer weights to zero and scale the output values by that factor.
# during testing, dropout is not active, the output will be scaled up instead.
dropout_factor=0
# batch mode decides if gradients are summed up or the mean is used for backpropagation.
# available values: mean|sum
batch_mode=mean

# ************************************************************************************************************ #
# ***********                                GENETIC ALGORITHM ONLY                                *********** #
# ************************************************************************************************************ #
# the parent count is the count of genea chosen for reproduction to be seeded to
# the new generation to come.
genetic_parent_count=1
# the selection pool size for genetic evolution as a percentage of the best performing neural networks. fallback will be 1.
genetic_reproduction_pool_size=0.5
# the crossover strategy. options: mean|slice
crossover_strategy=none
# the crossover slice count. must be integer larger than 0.
# as the matrices in the neural network (including biases) may have different dimensions, the slice count is not guaranteed.
# it may be regarded as a maximum slice count per operation.
crossover_slice_count=1
# the mutation rate is the percentage of the mutated components of the neural network matrices.
# must have a value between 0.0 and 1.0
mutation_rate=0.5
# the optimizer for the mutation rate between iterations.
# available values: none|sgd
mutation_rate_optimizer=sgd
# the mutation rate decay as momentum.
# if mutation_rate_optimizer is set to 'none', this value will have no effect.
# must have a value between 0.0 and 1.0.
mutation_rate_momentum=0.01

